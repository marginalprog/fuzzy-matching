# Анализ результатов бенчмарка

## Общий обзор

Было проведено расширенное тестирование производительности библиотеки fuzzy_matching с различными конфигурациями и размерами данных (100, 500 и 1000 записей). Тестирование включало:
- Базовые конфигурации с/без транслитерации
- Различные алгоритмы нечеткого сопоставления
- Оценку влияния блокировки
- Анализ масштабируемости

## Ключевые метрики

### Производительность базовых конфигураций

| Размер данных | Без транслитерации (сек) | С транслитерацией (сек) | Накладные расходы |
|---------------|-------------------------|------------------------|------------------|
| 100          | 0.0023                 | 0.0061                | 162.29%          |
| 500          | 0.0231                 | 0.0919                | 298.51%          |
| 1000         | 0.0765                 | 0.3589                | 369.02%          |

### Влияние блокировки на производительность

| Размер данных | С блокировкой (сек) | Без блокировки (сек) | Разница |
|---------------|---------------------|---------------------|----------|
| 100          | 0.0061             | 0.3070             | 4923%    |
| 500          | 0.0919             | 7.1508             | 7683%    |
| 1000         | 0.3589             | 27.5158            | 7566%    |

### Сравнение алгоритмов (для 1000 записей)

| Алгоритм | Без транслитерации (сек) | С транслитерацией (сек) | Найдено совпадений |
|----------|-------------------------|------------------------|-------------------|
| PARTIAL_RATIO | 0.1116             | 0.3827                | 443               |
| TOKEN_SORT   | 0.0895             | 0.3831                | 411               |
| TOKEN_SET    | 0.0999             | 0.3828                | 411               |
| WRatio       | 0.1181             | 0.4011                | 418               |

## Основные выводы

1. **Влияние транслитерации**:
   - Добавление транслитерации увеличивает время обработки в 3-4 раза
   - Накладные расходы растут с увеличением объема данных
   - Для 1000 записей время выполнения увеличивается на 369%

2. **Критичность блокировки**:
   - Блокировка критически важна для производительности
   - Без блокировки время выполнения возрастает в 50-77 раз
   - Эффективность блокировки увеличивается с ростом объема данных

3. **Сравнение алгоритмов**:
   - PARTIAL_RATIO находит больше всего совпадений (443 для 1000 записей)
   - TOKEN_SORT показывает лучшую производительность без транслитерации
   - WRatio имеет наибольшие накладные расходы, но обеспечивает баланс точности

4. **Масштабируемость**:
   - Без транслитерации: приблизительно O(n^3.3)
   - С транслитерацией: приблизительно O(n^5.9)
   - Значительное ухудшение производительности при больших объемах данных

## Рекомендации по использованию

1. **Выбор конфигурации**:
   - Для малых наборов данных (<500 записей) можно использовать любую конфигурацию
   - Для средних наборов (500-1000 записей) рекомендуется использовать блокировку
   - Для больших наборов (>1000 записей) необходимо тщательно оптимизировать конфигурацию

2. **Оптимизация производительности**:
   - Всегда использовать блокировку для больших наборов данных
   - Применять частичную транслитерацию только для необходимых полей
   - Выбирать алгоритм в зависимости от требований к точности:
     * TOKEN_SORT: для лучшей производительности
     * PARTIAL_RATIO: для максимального количества совпадений
     * WRatio: для сбалансированного подхода

3. **Рекомендации по алгоритмам**:
   - Для имен и фамилий: PARTIAL_RATIO
   - Для адресов и длинных строк: TOKEN_SET
   - Для коротких строк и точных совпадений: TOKEN_SORT
   - Для универсального использования: WRatio

4. **Оптимизация памяти**:
   - Использовать блокировку для уменьшения количества сравнений
   - Применять частичную транслитерацию
   - Очищать неиспользуемые данные после обработки

## Профилирование

Анализ профилирования показывает следующие узкие места:

1. **Транслитерация**:
   - Занимает до 70% времени выполнения
   - Основные затраты на:
     * Определение языка
     * Нормализацию имен
     * Регулярные выражения

2. **Вычисление схожести**:
   - _weighted_average_similarity: ~20% времени
   - _get_similarity: ~10% времени
   - Обработка блоков: ~5% времени

## Заключение

Библиотека показывает хорошую производительность для небольших и средних наборов данных. Основные факторы, влияющие на производительность:
1. Использование блокировки (критически важно)
2. Выбор алгоритма сопоставления
3. Применение транслитерации

Для оптимальной производительности рекомендуется:
1. Всегда использовать блокировку
2. Выбирать алгоритм сопоставления в зависимости от типа данных
3. Минимизировать использование транслитерации
4. Применять частичную транслитерацию только для необходимых полей

![Сравнение времени выполнения](algorithm_time_comparison.png)

![Сравнение найденных совпадений](algorithm_matches_comparison.png)

![Эффективность алгоритмов](algorithm_efficiency_comparison.png)

![Влияние блокировки](blocking_impact.png)

